# Final-Dissertation-Submission


This project was set up to work with HTC Vive headset using Unity (version 2018.4.32f1). 

Abstract: 

Virtual Reality headsets provide an interactive, immersive real-world simulation, 
which makes them a powerful rehabilitative tool. This project was a contribution 
towards the effort to find suitable navigational aids that could facilitate 
performance improvement of the patients with acquired brain injury on tasks that 
require finding a target in world-scale spaces (e.g. supermarket). One such task is
the Multiple Errands Tasks and its virtual version called VMET. The focus of this 
paper was placed on the comparison between visual, auditory and haptic cues, as 
well as cues that consisted of a combination of two modalities. Haptic and auditory 
feedback have lower bandwidth than visual cues, however, they also have a 
potential to be more discreet than visual feedback. A pilot study was conducted to 
gain insight into user preference and performance regarding a combination of 
modalities, and to choose the most suitable visual cue. A second experiment was 
designed to address the problems uncovered by the pilot study and iterate on the 
navigational aid designs. This included reducing the number of conditions in order 
to collect a larger amount of data to perform the most important comparison 
between the three basic modalities â€“ visual, auditory and haptic. Across both 
experiments, 27 participants were tested on 9 mazes in room-scale Virtual Reality, 
using an HTC Vive headset. It was found that arrows shown one after another to 
form a path to the target led to the shortest time and distance walked to complete 
the maze. This condition was also found to be significantly preferred over other 
types of cues on the questionnaire exploring user preference. These results, and 
their limitations, were then discussed against the context of prior work.
